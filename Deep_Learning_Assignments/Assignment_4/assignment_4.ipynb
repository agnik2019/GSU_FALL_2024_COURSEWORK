{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9vk9Elugvmi"
      },
      "source": [
        "# Assignment 4 - Graph Neural Networks\n",
        "\n",
        "In this assignment we will implement graph representation, graph classification, neighborhood sampling and graph attention components. This assignment is based on the Chapter 13 from the book [https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1 - Graph Representation (10 points)\n",
        "\n",
        "This notebook investigates representing graphs with matrices as illustrated in figure 13.4 from the book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLComQyvCIJ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1QMxC7X4vh9"
      },
      "outputs": [],
      "source": [
        "# Routine to draw graph structure\n",
        "def draw_graph_structure(adjacency_matrix):\n",
        "\n",
        "  G = nx.Graph()\n",
        "  n_node = adjacency_matrix.shape[0]\n",
        "  for i in range(n_node):\n",
        "    for j in range(i):\n",
        "      if adjacency_matrix[i,j]:\n",
        "          G.add_edge(i,j)\n",
        "\n",
        "  nx.draw(G, nx.spring_layout(G, seed = 0), with_labels=True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIrihEw-7DRV"
      },
      "outputs": [],
      "source": [
        "# Define a graph\n",
        "# Note that the nodes are labelled from 0 rather than 1 as in the book\n",
        "A = np.array([[0,1,0,1,0,0,0,0],\n",
        "     [1,0,1,1,1,0,0,0],\n",
        "     [0,1,0,0,1,0,0,0],\n",
        "     [1,1,0,0,1,0,0,0],\n",
        "     [0,1,1,1,0,1,0,1],\n",
        "     [0,0,0,0,1,0,1,1],\n",
        "     [0,0,0,0,0,1,0,0],\n",
        "     [0,0,0,0,1,1,0,0]]);\n",
        "print(A)\n",
        "draw_graph_structure(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzvfUpkV4zCj"
      },
      "outputs": [],
      "source": [
        "# 5 points\n",
        "# TODO -- find algorithmically how many walks of length three are between nodes 3 and 7\n",
        "# Replace this line\n",
        "print(\"Number of  walks between nodes three and seven = ???\")\n",
        "\n",
        "# 5 points\n",
        "# TODO: Find algorithmically how many paths of length 3 are there between node 0 and every other node\n",
        "# Replace this line\n",
        "x = np.zeros((A.shape[0],1))\n",
        "x[0] = 1\n",
        "print(\"Number of paths of length 3 from node 0 to every other node \\n\", np.zeros_like(x).reshape(1,-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 - Graph Classification - 20 points\n",
        "\n",
        "Let's build the components of a model that maps a chemical structure to a binary decision. This model might be used to predict whether a chemical is liquid at room temperature or not. We'll start by drawing the chemical structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a graph that represents the chemical structure of ethanol and draw it\n",
        "# Each node is labelled with the node number and the element (carbon, hydrogen, oxygen)\n",
        "G = nx.Graph()\n",
        "G.add_edge('0:H','2:C')\n",
        "G.add_edge('1:H','2:C')\n",
        "G.add_edge('3:H','2:C')\n",
        "G.add_edge('2:C','5:C')\n",
        "G.add_edge('4:H','5:C')\n",
        "G.add_edge('6:H','5:C')\n",
        "G.add_edge('7:O','5:C')\n",
        "G.add_edge('8:H','7:O')\n",
        "nx.draw(G, nx.spring_layout(G, seed = 0), with_labels=True, node_size=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5 points\n",
        "# Define adjacency matrix\n",
        "# TODO -- Define the adjacency matrix for this chemical\n",
        "# Replace this line\n",
        "A = np.zeros((9,9)) ;\n",
        "\n",
        "print(A)\n",
        "\n",
        "# TODO -- Define node matrix\n",
        "# There will be 9 nodes and 118 possible chemical elements\n",
        "# so we'll define a 118x9 matrix.  Each column represents one\n",
        "# node and is a one-hot vector (i.e. all zeros, except a single one at the\n",
        "# chemical number of the element).\n",
        "# Chemical numbers:  Hydrogen-->1, Carbon-->6,  Oxygen-->8\n",
        "# Since the indices start at 0, we'll set element 0 to 1 for hydrogen, element 5\n",
        "# to one for carbon, and element 7 to one for oxygen\n",
        "# Replace this line:\n",
        "X = np.zeros((118,9))\n",
        "\n",
        "# Print the top 15 rows of the data matrix\n",
        "print(X[0:15,:])\n",
        "draw_graph_structure(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define a network with four layers that maps this graph to a binary value, using the formulation in equation 13.11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll need these helper functions\n",
        "\n",
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation\n",
        "\n",
        "# Define the logistic sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1.0/(1.0+np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our network will have K=3 hidden layers, and will use a dimension of D=200.\n",
        "K = 3; D = 200\n",
        "# Set seed so we always get the same random numbers\n",
        "np.random.seed(1)\n",
        "# Let's initialize the parameter matrices randomly with He initialization\n",
        "Omega0 = np.random.normal(size=(D, 118)) * 2.0 / D\n",
        "beta0 = np.random.normal(size=(D,1)) * 2.0 / D\n",
        "Omega1 = np.random.normal(size=(D, D)) * 2.0 / D\n",
        "beta1 = np.random.normal(size=(D,1)) * 2.0 / D\n",
        "Omega2 = np.random.normal(size=(D, D)) * 2.0 / D\n",
        "beta2 = np.random.normal(size=(D,1)) * 2.0 / D\n",
        "omega3 = np.random.normal(size=(1, D))\n",
        "beta3 = np.random.normal(size=(1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def graph_neural_network(A, X, Omega0, beta0, Omega1, beta1, Omega2, beta2, omega3, beta3):\n",
        "  # TODO Define this network according to equation 13.11 from the book\n",
        "  # We will use ReLU as the nonlinear activation for the hidden layers\n",
        "  # We will use sigmoid activation for the last layer\n",
        "  # Replace this line\n",
        "  f = np.ones((1,1))\n",
        "  \n",
        "  return f;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10 points\n",
        "# Let's test this network\n",
        "f = graph_neural_network(A,X, Omega0, beta0, Omega1, beta1, Omega2, beta2, omega3, beta3)\n",
        "print(\"Your value is %3f: \"%(f[0,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5 points\n",
        "# Let's check that permuting the indices of the graph doesn't change\n",
        "# the output of the network\n",
        "# Define a permutation matrix\n",
        "P = np.array([[0,1,0,0,0,0,0,0,0],\n",
        "              [0,0,0,0,1,0,0,0,0],\n",
        "              [0,0,0,0,0,1,0,0,0],\n",
        "              [0,0,0,0,0,0,0,0,1],\n",
        "              [1,0,0,0,0,0,0,0,0],\n",
        "              [0,0,1,0,0,0,0,0,0],\n",
        "              [0,0,0,1,0,0,0,0,0],\n",
        "              [0,0,0,0,0,0,0,1,0],\n",
        "              [0,0,0,0,0,0,1,0,0]]);\n",
        "\n",
        "# TODO -- Use this matrix to permute the adjacency matrix A and node matrix X\n",
        "# Replace these lines\n",
        "A_permuted = np.copy(A)\n",
        "X_permuted = np.copy(X)\n",
        "\n",
        "f2 = graph_neural_network(A_permuted,X_permuted, Omega0, beta0, Omega1, beta1, Omega2, beta2, omega3, beta3)\n",
        "print(\"After permutation value is %3f: \"%(f2[0,0]), \"Before permutation value is %3f: \"%(f[0,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3 - Neighborhood Sampling - 40 points\n",
        "In this section we will investigate neighborhood sampling of graphs as in figure 13.10 from the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's construct the graph from figure 13.10, which has 23 nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define adjacency matrix\n",
        "# DO NOT change this matrix\n",
        "A = np.array([[0,1,1,1,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [1,0,1,0,0, 0,0,0,1,1, 0,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [1,1,0,1,0, 0,0,0,0,1, 0,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [1,0,1,0,1, 0,1,1,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [0,0,0,1,0, 1,0,1,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [0,0,0,0,1, 0,0,1,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [0,0,0,1,0, 0,0,1,0,1, 1,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [0,0,0,1,1, 1,1,0,0,0, 1,0,0,1,0, 0,0,0,0,0, 0,0,0],\n",
        "              [0,1,0,0,0, 0,0,0,0,1, 0,0,0,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [0,1,1,0,0, 0,1,0,1,0, 0,1,1,0,0, 0,1,0,0,0, 0,0,0],\n",
        "              [0,0,0,0,0, 0,1,1,0,0, 0,0,1,0,0, 0,0,0,0,0, 0,0,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,1, 0,0,0,0,1, 1,1,0,0,0, 0,0,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,1, 1,0,0,1,0, 0,1,1,0,0, 0,0,0],\n",
        "              [0,0,0,0,0, 0,0,1,0,0, 0,0,1,0,0, 0,0,1,1,0, 0,0,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,1,0,0,0, 1,0,0,0,1, 0,0,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,1,0,0,1, 0,1,0,0,1, 0,0,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,1, 0,1,1,0,0, 1,0,1,0,1, 0,0,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,0,1,1,0, 0,1,0,1,0, 1,1,1],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,0,0,1,0, 0,0,1,0,0, 0,0,1],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,1, 1,1,0,0,0, 1,0,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,1,0,1, 0,1,0],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,1,0,0, 1,0,1],\n",
        "              [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,1,1,0, 0,1,0]]);\n",
        "print(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Routine to draw graph structure, highlighting original node (brown in fig 13.10)\n",
        "# and neighborhood nodes (orange in figure 13.10)\n",
        "def draw_graph_structure(adjacency_matrix, original_node, neighborhood_nodes=None):\n",
        "\n",
        "  G = nx.Graph()\n",
        "  n_node = adjacency_matrix.shape[0]\n",
        "  for i in range(n_node):\n",
        "    for j in range(i):\n",
        "      if adjacency_matrix[i,j]:\n",
        "          G.add_edge(i,j)\n",
        "\n",
        "  color_map = []\n",
        "\n",
        "  for node in G:\n",
        "    if original_node[node]:\n",
        "      color_map.append('brown')\n",
        "    else:\n",
        "      if neighborhood_nodes[node]:\n",
        "        color_map.append('orange')\n",
        "      else:\n",
        "        color_map.append('white')\n",
        "\n",
        "  nx.draw(G, nx.spring_layout(G, seed = 7), with_labels=True,node_color=color_map)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_nodes = A.shape[0]\n",
        "\n",
        "# Define a single output layer node\n",
        "output_layer_nodes=np.zeros((n_nodes,1)); output_layer_nodes[16]=1\n",
        "# Define the neighboring nodes to draw (none)\n",
        "neighbor_nodes = np.zeros((n_nodes,1))\n",
        "print(\"Output layer:\")\n",
        "draw_graph_structure(A, output_layer_nodes, neighbor_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's imagine that we want to form a batch for a node labelling task that consists of just node 16 in the output layer (highlighted). The network consists of the input, hidden layer 1, hidden layer2, and the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4 points\n",
        "# TODO Find the nodes in hidden layer 2 that connect to node 16 in the output layer\n",
        "# using the adjacency matrix\n",
        "# Replace this line:\n",
        "hidden_layer2_nodes = np.zeros((n_nodes,1));\n",
        "\n",
        "\n",
        "print(\"Hidden layer 2:\")\n",
        "draw_graph_structure(A, output_layer_nodes, hidden_layer2_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3 points\n",
        "# TODO - Find the nodes in hidden layer 1 that connect that connect to node 16 in the output layer\n",
        "# using the adjacency matrix\n",
        "# Replace this line:\n",
        "hidden_layer1_nodes = np.zeros((n_nodes,1));\n",
        "\n",
        "print(\"Hidden layer 1:\")\n",
        "draw_graph_structure(A, output_layer_nodes, hidden_layer1_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3 points\n",
        "# TODO Find the nodes in the input layer that connect to node 16 in the output layer\n",
        "# using the adjacency matrix\n",
        "# Replace this line:\n",
        "input_layer_nodes = np.zeros((n_nodes,1));\n",
        "\n",
        "print(\"Input layer:\")\n",
        "draw_graph_structure(A, output_layer_nodes, input_layer_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is bad news. This is a fairly sparsely connected graph (i.e. adjacency matrix is mostly zeros) and there are only two hidden layers. Nonetheless, we have to involve almost all the nodes in the graph to compute the loss at this output.\n",
        "\n",
        "To resolve this problem, we'll use neighborhood sampling. We'll start again with a single node in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Output layer:\")\n",
        "draw_graph_structure(A, output_layer_nodes, neighbor_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define number of neighbors to sample\n",
        "n_sample = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10 points\n",
        "# TODO Find the nodes in hidden layer 2 that connect to node 16 in the output layer\n",
        "# using the adjacency matrix.  Then sample n_sample of these nodes randomly without\n",
        "# replacement.\n",
        "\n",
        "# Replace this line:\n",
        "hidden_layer2_nodes = np.zeros((n_nodes,1));\n",
        "\n",
        "draw_graph_structure(A, output_layer_nodes, hidden_layer2_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10 points\n",
        "# TODO Find the nodes in hidden layer 1 that connect to the nodes in hidden layer 2\n",
        "# using the adjacency matrix.  Then sample n_sample of these nodes randomly without\n",
        "# replacement.  Make sure not to sample nodes that were already included in hidden layer 2, or the output layer.\n",
        "# The nodes at hidden layer 1 are the union of these nodes and the nodes in hidden layer 2\n",
        "\n",
        "# Replace this line:\n",
        "hidden_layer1_nodes = np.zeros((n_nodes,1));\n",
        "\n",
        "draw_graph_structure(A, output_layer_nodes, hidden_layer1_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10 points\n",
        "# TODO Find the nodes in the input layer that connect to the nodes in hidden layer 1\n",
        "# using the adjacency matrix.  Then sample n_sample of these nodes randomly without\n",
        "# replacement.  Make sure not to sample nodes that were already included in hidden layer 1,2, or the output layer.\n",
        "# The nodes at the input layer are the union of these nodes and the nodes in hidden layers 1 and 2\n",
        "\n",
        "# Replace this line:\n",
        "input_layer_nodes = np.zeros((n_nodes,1));\n",
        "\n",
        "draw_graph_structure(A, output_layer_nodes, input_layer_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you did this correctly, there should be 9 yellow nodes in the figure. The \"receptive field\" of node 16 in the output layer increases much more slowly as we move back through the layers of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4 - Graph Attention Network - 30 points\n",
        "\n",
        "In this part we will build a graph attention mechanism from scratch, as discussed in section 13.8.6 of the book and illustrated in figure 13.12c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(1)\n",
        "# Number of nodes in the graph\n",
        "N = 8\n",
        "# Number of dimensions of each input\n",
        "D = 4\n",
        "\n",
        "# Define a graph\n",
        "A = np.array([[0,1,0,1,0,0,0,0],\n",
        "              [1,0,1,1,1,0,0,0],\n",
        "              [0,1,0,0,1,0,0,0],\n",
        "              [1,1,0,0,1,0,0,0],\n",
        "              [0,1,1,1,0,1,0,1],\n",
        "              [0,0,0,0,1,0,1,1],\n",
        "              [0,0,0,0,0,1,0,0],\n",
        "              [0,0,0,0,1,1,0,0]]);\n",
        "print(A)\n",
        "\n",
        "# Let's also define some random data\n",
        "X = np.random.normal(size=(D,N))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need parameters for the graph attention layer. See Equation 13.22 - 13.24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose random values for the parameters\n",
        "omega = np.random.normal(size=(D,D))\n",
        "beta = np.random.normal(size=(D,1))\n",
        "phi = np.random.normal(size=(2*D,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll need a softmax operation that operates on the columns of the matrix and a ReLU function as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # Replicate denominator to N rows\n",
        "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
        "  # Compute softmax\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax\n",
        "\n",
        "\n",
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 30 points \n",
        "# Now let's compute self attention in matrix form\n",
        "def graph_attention(X,omega, beta, phi, A):\n",
        "\n",
        "  # TODO -- Write this function (see figure 13.12c)\n",
        "  # 1. Compute X_prime\n",
        "  # 2. Compute S\n",
        "  # 3. To apply the mask, set S to a very large negative number (e.g. -1e20) everywhere where A+I is zero\n",
        "  # 4. Run the softmax function to compute the attention values\n",
        "  # 5. Postmultiply X' by the attention values\n",
        "  # 6. Apply the ReLU function\n",
        "  # Replace this line:\n",
        "  output = np.ones_like(X) ;\n",
        "\n",
        "  return output;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Test out the graph attention mechanism\n",
        "np.set_printoptions(precision=3)\n",
        "output = graph_attention(X, omega, beta, phi, A);\n",
        "print(\"Correct answer is:\")\n",
        "print(\"[[0.    0.028 0.37  0.    0.97  0.    0.    0.698]\")\n",
        "print(\" [0.    0.    0.    0.    1.184 0.    2.654 0.  ]\")\n",
        "print(\" [1.13  0.564 0.    1.298 0.268 0.    0.    0.779]\")\n",
        "print(\" [0.825 0.    0.    1.175 0.    0.    0.    0.  ]]]\")\n",
        "\n",
        "\n",
        "print(\"Your answer is:\")\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMuzP1/oqTRTw4Xs/R4J/M3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
